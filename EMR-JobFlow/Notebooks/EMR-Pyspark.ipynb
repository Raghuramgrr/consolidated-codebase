{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import tarfile\n",
    "import boto3\n",
    "import botocore\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_logging(default_level=logging.WARNING):\n",
    "    \"\"\"\n",
    "    Setup logging configuration\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=default_level)\n",
    "    return logging.getLogger('TriggerSparkSubmit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def terminate(error_message=None):\n",
    "    \"\"\"\n",
    "    Method to exit the Python script. It will log the given message and then exit().\n",
    "    :param error_message:\n",
    "    \"\"\"\n",
    "    if error_message:\n",
    "        logger.error(error_message)\n",
    "    logger.critical('The script is now terminating')\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intialize():\n",
    "        app_name = \"unique_visitors_spark\"                  # Application name\n",
    "        ec2_key_name = \"dynobjx\"                       # Key name to use for cluster\n",
    "        job_flow_id = \"j-2A3GSR7FH1UE0\"                # Returned by AWS in start_spark_cluster()\n",
    "        job_name = None                                # Filled by generate_job_name()\n",
    "        path_script = \"/home/ab/pyspark/spark_demo/\"                 # Path of Spark script to be deployed on AWS Cluster\n",
    "        s3_bucket_logs = \"s3://aws-logs-507340184449-ap-southeast-1/elasticmapreduce/\"   # S3 Bucket to store AWS EMR logs\n",
    "        s3_bucket_temp_files = \"daimlerdemotemp\"     # S3 Bucket to store temporary files\n",
    "        s3_region = 's3-ap-southeast-1-amazonaws.com'       # S3 region to specifiy s3Endpoint in s3-dist-cp step\n",
    "        user = 'Raghu'                                  # Define user name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_job_name(self):\n",
    "        self.job_name = \"{}.{}.{}\".format(self.app_name,\n",
    "                                          self.user,\n",
    "                                          datetime.now().strftime(\"%Y%m%d.%H%M%S.%f\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def temp_bucket_exists(self, s3):\n",
    "        \"\"\"\n",
    "        Check if the bucket we are going to use for temporary files exists.\n",
    "        :param s3:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            s3.meta.client.head_bucket(Bucket=self.s3_bucket_temp_files)\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            # If a client error is thrown, then check that it was a 404 error.\n",
    "            # If it was a 404 error, then the bucket does not exist.\n",
    "            error_code = int(e.response['Error']['Code'])\n",
    "            if error_code == 404:\n",
    "                terminate(\"Bucket for temporary files does not exist\")\n",
    "            terminate(\"Error while connecting to Bucket\")\n",
    "        logger.info(\"S3 bucket for temporary files exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  def tar_python_script(self):\n",
    "        \"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Create tar.gz file\n",
    "        t_file = tarfile.open(\"spark_demo/script.tar.gz\", 'w:gz')\n",
    "        # Add Spark script path to tar.gz file\n",
    "        files = os.listdir(self.path_script)\n",
    "        for f in files:\n",
    "            t_file.add(self.path_script + f, arcname=f)\n",
    "        # List all files in tar.gz\n",
    "        for f in t_file.getnames():\n",
    "            logger.info(\"Added %s to tar-file\" % f)\n",
    "        t_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "      \n",
    "    #$ command-runner.jar spark-submit --deploy-mode cluster --conf PYSPARK_PYTHON=/usr/bin/python3 \n",
    "    #s3://mybucket/mypath/myscript.py\n",
    "    \n",
    "def intialize_s3_toSpark(self,c):\n",
    "            Applications=[{'Name': 'Hadoop'}, {'Name': 'Spark'}],\n",
    "            JobFlowRole='EMR_EC2_DefaultRole',\n",
    "            ServiceRole='EMR_DefaultRole',\n",
    "            VisibleToAllUsers=True,\n",
    "            BootstrapActions=[{\n",
    "            'Name': 'Setup',\n",
    "            'ScriptBootstrapAction': {\n",
    "                'path':'s3n://daimlerdemotemp/setup.sh'.format(self.s3_bucket_temp_files, self.job_name),\n",
    "                'Args': ['s3://daimlerdemotemp/'.format(self.s3_bucket_temp_files, self.job_name)\n",
    "                       ]\n",
    "            }\n",
    "        }]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def step_spark_submit(self, c, arguments):\n",
    "        \n",
    "        \n",
    "        response = c.add_job_flow_steps(\n",
    "        JobFlowId=\"j-2A3GSR7FH1UE0\",\n",
    "        Steps=[{\n",
    "            'Name': 'Spark Application',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "               'Jar': 'command-runner.jar',\n",
    "               'Args': [\"spark-submit\", \"/home/hadoop/unique_counts.py\", arguments]\n",
    "            }\n",
    "        }]\n",
    "    )\n",
    "        logger.info(\"Added step 'spark-submit' with argument '{}'\".format(arguments))\n",
    "        time.sleep(1) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upload_temp_files(self, s3):\n",
    "        \"\"\"\n",
    "        Move the PySpark script files to the S3 bucket we use to store temporary files\n",
    "        :param s3:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        s3.Object(self.s3_bucket_temp_files, self.job_name + '/setup.sh')\\\n",
    "          .put(Body=open('spark_demo/setup.sh', 'rb'), ContentType='text/x-sh')\n",
    "            \n",
    "        s3.Object(self.s3_bucket_temp_files, self.job_name + '/script.tar.gz')\\\n",
    "          .put(Body=open('spark_demo/script.tar.gz', 'rb'), ContentType='application/x-tar')\n",
    "        logger.info(\"Uploaded files to key '{}' in bucket '{}'\".format(self.job_name, self.s3_bucket_temp_files))\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(self):\n",
    "        logger=setup_logging()\n",
    "        session = boto3.Session()                     # Select AWS IAM profile\n",
    "        s3 = session.resource('s3')                   # Open S3 connection\n",
    "        generate_job_name()                            # Generate job name\n",
    "        temp_bucket_exists(s3)  \n",
    "        tar_python_script()\n",
    "        upload_temp_files(s3)                          #uploads local file to temp s3\n",
    "        c = session.client('emr')                           # Open EMR connection \n",
    "        intialize_s3_toSpark(c)\n",
    "        step_spark_submit(c,\"\")                        # Add step 'spark-submit'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
