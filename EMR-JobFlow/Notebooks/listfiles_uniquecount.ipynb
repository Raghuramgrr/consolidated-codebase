{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qadams analysis_outputs\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from datetime import timedelta\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql .types import *\n",
    "import json \n",
    "import datetime\n",
    "\n",
    "#params = '{\"dataset\":{\"type\":\".csv\", \"prefix\":\"event_count/partition\", \"folder\": \"event_count/\", \"bucket\": \"daimler-phase1\"}, \"master_node\": \"local\", \"filters\": [{\"index\": 369, \"values\": [\"Registration\"]},{\"index\": 370, \"values\": [\"Failed\", \"Request Success\"]}], \"file\": \"output.csv\", \"application_name\":\"event_count\", \"datetype\": \"month\", \"daterange\": [\"2016-01-01\", \"2017-05-05\"], \"unique\": true, \"session_time\": [23], \"events\": [369, 370], \"session_id\": [956, 957, 962]}'\n",
    "#params = open(params.json).read()\n",
    "sc = SparkContext.getOrCreate()\n",
    "inputs = json.load(open('params.json'))\n",
    "OUTPUT_DATA_FOLDER=inputs [\"dataset\"][\"outputfolder\"]\n",
    "\n",
    "\n",
    "INPUT_DATA_BUCKET=inputs[\"dataset\"]['bucket']\n",
    "INPUT_DATA_FOLDER=inputs[\"dataset\"]['folder']\n",
    "#UNIQUE = inputs[\"unique\"]\n",
    "INPUT_DATA_PREFIX=inputs[\"dataset\"]['prefix']\n",
    "OUTPUT_DATA_BUCKET= inputs[\"dataset\"][\"outputbucket\"]\n",
    "INPUT_DATA_TYPE =inputs[\"dataset\"]['type']\n",
    "EVENTS = inputs[\"events\"]\n",
    "FILTERS = inputs[\"filters\"]\n",
    "UNIQUE = inputs[\"unique\"] \n",
    "DATERANGE = inputs[\"daterange\"] \n",
    "DATETYPE= inputs[\"datetype\"] \n",
    "SESSION_ID=inputs[\"session_id\"] \n",
    "SESSION_TIME= inputs[\"session_time\"] \n",
    "UPDATE_ID = inputs[\"update-id\"]\n",
    "HEADERS = {}\n",
    "IS_INIT = False\n",
    "DUMMY_DATA = \"upload_dummy\"\n",
    "print OUTPUT_DATA_BUCKET,OUTPUT_DATA_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def file_mapper(line):\n",
    "        lines = line[1].split(\"\\n\")\n",
    "        if not IS_INIT:\n",
    "            return lines\n",
    "        else:\n",
    "            return lines[1:]\n",
    "\n",
    "    def event_mapper(line):\n",
    "   \n",
    "        line = line.split(\",\")\n",
    "\n",
    "        if len(HEADERS.keys()) == 0:\n",
    "            for i in range(0, len(line)):\n",
    "                HEADERS[int(line[i])] = i\n",
    "            return line\n",
    "        if len(line) != len(HEADERS.keys()):\n",
    "            return (\"ERROR\", [])\n",
    "        HEADERSS = {962: 10, 69: 2, 61: 1, 369: 3, 370: 4, 23: 0, 376: 5, 377: 6, 378: 7, 956: 8, 957: 9}\n",
    "        key = []\n",
    "        value = []\n",
    "        for _id in SESSION_ID:\n",
    "            key.append(line[HEADERSS[_id]])\n",
    "        for event in EVENTS:\n",
    "            key.append(line[HEADERSS[event]])\n",
    "\n",
    "        for filtr in FILTERS:\n",
    "            value.append(line[HEADERSS[filtr[\"index\"]]])\n",
    "\n",
    "        for tme in SESSION_TIME:\n",
    "            value.append(convert_timestamp_to_datetype(int(line[HEADERSS[tme]])))\n",
    "            value.append(int(line[HEADERSS[tme]]))\n",
    "\n",
    "        return (\"_\".join(key), value)\n",
    "    def filter_events(x):\n",
    "        if x[0] == \"ERROR\":\n",
    "            return False\n",
    "        hasEvent = True\n",
    "        x = x[1]\n",
    "#     for i in range(0, len(FILTERS)):\n",
    "#         if x[i] not in FILTERS[i][\"values\"]:\n",
    "#             return False\n",
    "#     if int(x[len(FILTERS)+1]) < daterange_to_timestamp(DATERANGE[0]) or int(x[len(FILTERS)+1]) > daterange_to_timestamp(DATERANGE[1]):\n",
    "#         return False\n",
    "        return True\n",
    "    def daterange_to_timestamp(date):\n",
    "        _dt = datetime.datetime.strptime(date, '%Y-%M-%d')\n",
    "        return int((_dt-datetime.datetime(1970,1,1)).total_seconds())\n",
    "\n",
    "    def convert_timestamp_to_datetype(timestamp):\n",
    "        date_type = \"%b %Y\"\n",
    "        if DATETYPE == \"month\":\n",
    "            date_type = \"%b %Y\"\n",
    "        if DATETYPE == \"week\":\n",
    "            date_type = \"Week %W, %Y\"\n",
    "        if DATETYPE == \"day\":\n",
    "            date_type = \"%Y-%M-%d\"\n",
    "        if DATETYPE == \"weekday\":\n",
    "            date_type = \"%A\"\n",
    "        if DATETYPE == \"hour\":\n",
    "            date_type = \"%Y-%M-%d %I%p\"\n",
    "        if DATETYPE == \"hourday\":\n",
    "            date_type = \"%I%p\"\n",
    "        return datetime.datetime.fromtimestamp(timestamp).strftime(date_type)\n",
    "    \n",
    "    def upload(file,bucket, key):\n",
    "        #print bucket,key\n",
    "        s3 = boto3.resource('s3')\n",
    "        data = open(file, 'rb')\n",
    "        s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "        \n",
    "    def format_s3_partition_key(key):\n",
    "        return OUTPUT_DATA_FOLDER+\"/\"+key+\"_\"+UPDATE_ID\n",
    "    def format_params_partition_key(key):\n",
    "        return DUMMY_DATA+\"/\"+key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class event_count:\n",
    "    \n",
    "  #  params = '{\"dataset\":{\"type\":\".csv\", \"prefix\":\"event_count/partition\", \"folder\": \"event_count/\", \"bucket\": \"daimler-phase1\"}, \"master_node\": \"local\", \"filters\": [{\"index\": 369, \"values\": [\"Registration\"]},{\"index\": 370, \"values\": [\"Failed\", \"Request Success\"]}], \"file\": \"output.csv\", \"application_name\":\"event_count\", \"datetype\": \"month\", \"daterange\": [\"2016-01-01\", \"2017-05-05\"], \"unique\": true, \"session_time\": [23], \"events\": [369, 370], \"session_id\": [956, 957, 962]}'\n",
    "    \n",
    "    def list():\n",
    "        client = boto3.client('s3')\n",
    "        list_objects=client.list_objects(Bucket=INPUT_DATA_BUCKET, Prefix=INPUT_DATA_PREFIX, Delimiter='/')\n",
    "        list_objects = list_objects[\"Contents\"]\n",
    "        objects = []\n",
    "        for obj in list_objects:\n",
    "              objects.append(obj['Key'])\n",
    "        return objects\n",
    "    \n",
    "    def map_by_date(x):\n",
    "            key = x[0].split(\"_\")\n",
    "            key = key[len(SESSION_ID):]\n",
    "            key = [x[1][len(FILTERS)]]+ key\n",
    "            return (\"_\".join(key), 1)\n",
    "\n",
    "    def merge_keys(a,b):\n",
    "        return a\n",
    "\n",
    "    def format_result(x):\n",
    "        key = x[0].split(\"_\")\n",
    "        label = key[0]\n",
    "        serie = \" \".join(key[1:])\n",
    "        count = x[1]\n",
    "        return (serie, (label, count))\n",
    "\n",
    " \n",
    "    \n",
    "    def list_fileNames(self):\n",
    "        client = boto3.client('s3')\n",
    "        list_objects=client.list_objects(Bucket=INPUT_DATA_BUCKET, Prefix=INPUT_DATA_PREFIX, Delimiter='/')\n",
    "        list_objects = list_objects[\"Contents\"]\n",
    "        objects = []\n",
    "        for obj in list_objects:\n",
    "              objects.append(obj['Key'])\n",
    "        return objects\n",
    "\n",
    "    \n",
    "    def csv2data(self):\n",
    "        lists=self.list_fileNames()\n",
    "        print lists\n",
    "        csvFiles=[]\n",
    "        for l in lists:\n",
    "               csvFiles.append(l.encode('utf-8').split('/')[1])\n",
    "\n",
    "                \n",
    "        s3 = boto3.client('s3')\n",
    "        for c in csvFiles:\n",
    "            upload(\"params.json\",INPUT_DATA_BUCKET,format_params_partition_key('params.json'))\n",
    "            s3.download_file(INPUT_DATA_BUCKET,INPUT_DATA_FOLDER+c, \"unique_count\"+INPUT_DATA_TYPE)\n",
    "            #s3.download_file('s3://'INPUT_DATA_BUCKET'/'INPUT_DATA_FOLDER'/'+c,\"unique_count\"+INPUT_DATA_TYPE)\n",
    "            datas = sc.wholeTextFiles(\"unique_count\"+INPUT_DATA_TYPE)\n",
    "            #datas=sc.wholeTextFiles('s3n://'+INPUT_DATA_BUCKET+'/'+INPUT_DATA_FOLDER+'/'+c)\n",
    "            sessions = datas.flatMap(file_mapper)\\\n",
    "                                 .map(event_mapper)\\\n",
    "                                 .filter(filter_events)\n",
    "            output={ \"data\": [] }\n",
    "            for s in sessions.collect():\n",
    "                output[\"data\"].append({ \"serie\": s[0], \"data\": s[1] })\n",
    "            with open('event_unique.json', 'wb') as f:\n",
    "                json.dump(output, f)\n",
    "                #print json.dumps(output)\n",
    "\n",
    "        upload(\"event_unique.json\",OUTPUT_DATA_BUCKET,format_s3_partition_key('event_unique.json'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'event_count/partition0.csv', u'event_count/partition1.csv', u'event_count/partition10.csv', u'event_count/partition11.csv', u'event_count/partition2.csv', u'event_count/partition3.csv', u'event_count/partition4.csv', u'event_count/partition5.csv', u'event_count/partition6.csv', u'event_count/partition7.csv', u'event_count/partition8.csv', u'event_count/partition9.csv']\n"
     ]
    }
   ],
   "source": [
    "eventcount = event_count()\n",
    "\n",
    "eventcount.csv2data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
